stages:
  - checkout
  - validate
  - build
  - test
  - docker
  - security-scan
  - deploy
  - deploy-staging
  - deploy-production
  - deploy-gke
  - monitoring-verify

variables:
  #versioning
  COMMIT_TAG: "${CI_COMMIT_SHORT_SHA}"
  # GitLab Container Registry variables
  GITLAB_REGISTRY_URL: "$CI_REGISTRY"
  GITLAB_REGISTRY_IMAGE: "$CI_REGISTRY_IMAGE"
  GITLAB_REGISTRY_USER: "$CI_REGISTRY_USER"
  GITLAB_REGISTRY_PASSWORD: "$CI_REGISTRY_PASSWORD"
  STAGING_NAMESPACE: staging

  #GCP/GKE specific variables
  GCP_PROJECT_ID: "decent-vertex-459014-k2"
  GKE_CLUSTER_NAME: "my-pfe-cluster"
  GKE_ZONE: "europe-west1-b"
  GKE_FRONTEND_NAMESPACE: "monitoring-frontend"
  GKE_REPLICAS: 2
  GKE_MEMORY_REQUEST: "256Mi"
  GKE_MEMORY_LIMIT: "512Mi"
  GKE_CPU_REQUEST: "100m"
  GKE_CPU_LIMIT: "200m"

  # Node.js and npm settings
  NODE_VERSION: "18"
  NPM_CONFIG_CACHE: "${CI_PROJECT_DIR}/.npm"
  CYPRESS_CACHE_FOLDER: "${CI_PROJECT_DIR}/.cypress"

  


# Job Templates
.job_template: &job_definition
  tags:
    - docker
  before_script:
    - echo "starting job in ${CI_JOB_NAME} stage"

.node_job: &node_definition
  <<: *job_definition
  image: node:18-alpine
  cache:
    key: npm-cache
    paths:
      - .npm/
      - node_modules/
  before_script:
    - echo "Setting up Node.js environment..."
    - npm config set cache $NPM_CONFIG_CACHE --global
    - node --version
    - npm --version
.kubectl_job: &kubectl_definition
  <<: *job_definition
  image: alpine/k8s:1.25.10
  variables:
    KUBECONFIG: /tmp/kubeconfig/config

# Template for production jobs
.production_job: &production_job_definition
  tags:
    - production
    - gitlab-runner-prod
    - kubernetes
    - minikube
    - registry
  variables:
    TARGET_ENV: "production"
    REGISTRY_URL: "$PRODUCTION_REGISTRY_URL"
    KUBE_SERVER: "https://$PRODUCTION_VM_IP:6443"

# Common deployment base
.monitoring_frontend_deploy_base: &monitoring_frontend_deploy_base
  <<: *job_definition
  image: alpine/k8s:1.30.0
  variables:
    APP_NAME: "monitoring-frontend"
    CONTAINER_PORT: 8080
    SERVICE_PORT: 80
    KUSTOMIZE_VERSION: "v5.4.3"
  before_script:
    - apk add --no-cache curl jq gettext
    - echo "Installing Kustomize..."
    - curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
    - mv kustomize /usr/local/bin/
    - kustomize version

.monitoring_staging_deploy: &monitoring_staging_deploy
  <<: *monitoring_frontend_deploy_base
  variables:
    ENVIRONMENT: staging
    KUBECONFIG: "$CI_PROJECT_DIR/.kube/config"
    MINIKUBE_IP: "192.168.49.2"
    MINIKUBE_PORT: "8443"
    KUBECTL_VERSION: "v1.32.3"
  before_script:
    - curl -LO "https://dl.k8s.io/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
    - chmod +x kubectl && mv kubectl /usr/local/bin/
    - mkdir -p "$(dirname "$KUBECONFIG")"
    - |
      echo "Creating kubeconfig with service account token..."
      cat > "$KUBECONFIG" <<EOF
      apiVersion: v1
      clusters:
      - cluster:
          server: https://${MINIKUBE_IP}:${MINIKUBE_PORT}
          insecure-skip-tls-verify: true
        name: minikube
      contexts:
      - context:
          cluster: minikube
          user: gitlab-ci
        name: minikube
      current-context: minikube
      kind: Config
      preferences: {}
      users:
      - name: gitlab-ci
        user:
          token: ${MINIKUBE_TOKEN}
      EOF
    - echo "Testing Kubernetes connection..."
    - kubectl cluster-info
    - kubectl get nodes
    - echo "✅ Kubernetes connection successful!"

.monitoring_production_deploy: &monitoring_production_deploy
  <<: *monitoring_frontend_deploy_base
  variables:
    ENVIRONMENT: production
    KUBECONFIG: "${CI_PROJECT_DIR}/.kube/config"
  before_script:
    - mkdir -p "$(dirname "$KUBECONFIG")"
    - |
      cat > "${KUBECONFIG}" <<EOF
      apiVersion: v1
      clusters:
      - cluster:
          server: ${K3S_SERVER}
          certificate-authority-data: ${K3S_CA_CERT}
        name: k3s-cluster
      contexts:
      - context:
          cluster: k3s-cluster
          user: k3s-user
        name: k3s-context
      current-context: k3s-context
      kind: Config
      preferences: {}
      users:
      - name: k3s-user
        user:
          client-certificate-data: ${K3S_CLIENT_CERT}
          client-key-data: ${K3S_CLIENT_KEY}
      EOF
    - kubectl config current-context
    - kubectl cluster-info

.monitoring_gke_deploy: &monitoring_gke_deploy
  <<: *monitoring_frontend_deploy_base
  image: google/cloud-sdk:slim
  variables:
    ENVIRONMENT: gke
    KUBECONFIG: "${CI_PROJECT_DIR}/.kube/gke-config"
    USE_GKE_GCLOUD_AUTH_PLUGIN: "True"
    CLOUDSDK_CORE_PROJECT: "$GCP_PROJECT_ID"
  before_script:
    - apt-get update && apt-get install -y --no-install-recommends curl kubectl tar google-cloud-cli-gke-gcloud-auth-plugin
    - echo "Installing Kustomize safely..."
    - export KUSTOMIZE_VERSION=v5.4.3
    - curl -Lo kustomize.tar.gz https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize%2F${KUSTOMIZE_VERSION}/kustomize_${KUSTOMIZE_VERSION}_linux_amd64.tar.gz
    - tar -zxvf kustomize.tar.gz
    - chmod +x kustomize && mv kustomize /usr/local/bin/
    - kustomize version
    - echo "Setting up GCP authentication..."
    - echo "$GCP_SERVICE_ACCOUNT_KEY" | base64 -d > /tmp/gcp-key.json
    - gcloud auth activate-service-account --key-file /tmp/gcp-key.json
    - gcloud config set project $GCP_PROJECT_ID
    - export CLOUDSDK_CORE_PROJECT=$GCP_PROJECT_ID
    - gcloud services enable container.googleapis.com cloudresourcemanager.googleapis.com
    - gcloud container clusters get-credentials $GKE_CLUSTER_NAME --zone $GKE_ZONE
  after_script:
    - rm -f /tmp/gcp-key.json




# Jobs
checkout:
  <<: *job_definition
  stage: checkout
  before_script:
    - echo "Cleaning previous build artifacts..."
    - rm -rf dist/ node_modules/ .angular/
  script:
    - echo "Checking out code from ${CI_REPOSITORY_URL}"
  artifacts:
    paths:
      - ./

# === VALIDATION JOBS ===

validate_package_json:
  <<: *node_definition
  stage: validate
  script:
    - echo "Validating package.json..."
    - apk add --no-cache jq
    - npm config list
    - cat package.json | jq empty  # Validate JSON syntax
    - echo "✅ package.json is valid"
    - echo "Checking for security vulnerabilities..."
    - npm audit --audit-level moderate || true  # Don't fail pipeline on audit errors
    - echo "✅ No high/critical vulnerabilities found"
  cache:
    policy: pull
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "develop"'
    - if: '$CI_COMMIT_BRANCH == "main"'

lint_code:
  <<: *node_definition
  stage: validate
  script:
    - echo "Installing dependencies for linting..."
    - npm ci 
    - echo "Running ESLint..."
    - npx ng lint  || true
    - echo "✅ Linting complete"
  allow_failure: true  # Don't block pipeline on lint failures
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "develop"'
    - if: '$CI_COMMIT_BRANCH == "main"'

build:
  <<: *node_definition
  stage: build
  script:
    - echo "Installing dependencies..."
    - npm ci
    - echo "Building Angular application..."
    - npm run build:stats
    - echo "Build completed successfully!"
    - echo "Listing build artifacts:"
    - ls -la dist/
    - du -sh dist/
    - echo "checking build output..."
    - test -f dist/browser/index.html || (echo "dist/index.html not found!" && exit 1)
    - echo "Build output is valid."
  artifacts:
    paths:
      - dist/
      - package.json
      - package-lock.json
      - node_modules/
    reports:
      dotenv: build.env
    expire_in: 2 hours
  after_script:
    - echo "BUILD_VERSION=${CI_COMMIT_SHORT_SHA}" > build.env
    - echo "BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ')" >> build.env
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "develop"'
    - if: '$CI_COMMIT_BRANCH == "main"'

# === TEST JOBS ===


build_size_analysis:
  <<: *node_definition
  stage: test
  needs: ["build"]
  script:
    - echo "Analyzing bundle sizes with source-map-explorer..."
    - npm ci
    - npx source-map-explorer dist/browser/*.js --html > bundle-report.html
    - echo "✅ Build analysis complete"
  artifacts:
    paths:
      - bundle-report.html
    expire_in: 1 week
  allow_failure: true
  rules:
    - if: '$CI_COMMIT_BRANCH == "develop"'
    - if: '$CI_COMMIT_BRANCH == "main"'





docker_publish:
  <<: *job_definition
  stage: docker
  image: docker:24.0
  variables:
    DOCKER_HOST: "unix:///var/run/docker.sock"
    DOCKER_BUILDKIT: 1
  before_script:
    - apk add --no-cache curl
    - echo "Using GitLab Container Registry $CI_REGISTRY_IMAGE"
    - echo "Commit SHA ${CI_COMMIT_SHORT_SHA}"
  script:
    - echo "=== BUILDING DOCKER IMAGE ==="
    - docker build -t "$CI_REGISTRY_IMAGE:${CI_COMMIT_SHORT_SHA}" -t "$CI_REGISTRY_IMAGE:latest" .

    - echo "=== LOGGING INTO GITLAB CONTAINER REGISTRY ==="
    - echo "$CI_REGISTRY_PASSWORD" | docker login "$CI_REGISTRY" -u "$CI_REGISTRY_USER" --password-stdin

    - echo "=== PUSHING IMAGES ==="
    - docker push "$CI_REGISTRY_IMAGE:${CI_COMMIT_SHORT_SHA}"
    - docker push "$CI_REGISTRY_IMAGE:latest"
  rules:
    - if: $CI_COMMIT_BRANCH == "develop"  # ou "main" si tu veux le faire sur main.

# ============= SECURITY SCANNING =============

container_security_scan:
  <<: *job_definition
  stage: security-scan
  image:
    name: aquasec/trivy:latest
    entrypoint: [""]
  needs: ["docker_publish"]
  variables:
    TRIVY_NO_PROGRESS: "true"
    TRIVY_CACHE_DIR: ".trivycache/"
  script:
    - echo "Scanning container image for vulnerabilities..."
    - trivy image --exit-code 0 --severity HIGH,CRITICAL --format table "$CI_REGISTRY_IMAGE:${CI_COMMIT_SHORT_SHA}"
    - trivy image --exit-code 1 --severity CRITICAL --format json --output trivy-report.json "$CI_REGISTRY_IMAGE:${CI_COMMIT_SHORT_SHA}"
    - echo "✅ Security scan completed"
  artifacts:
    reports:
      junit: trivy-report.json
    paths:
      - trivy-report.json
    expire_in: 1 week
  allow_failure: true  # Don't block pipeline initially
  rules:
    - if: '$CI_COMMIT_BRANCH == "develop" || $CI_COMMIT_BRANCH == "main"'

dependency_security_audit:
  <<: *node_definition
  stage: security-scan
  needs: ["build"]
  script:
    - apk add --no-cache jq
    - echo "Running security audit on dependencies..."
    - npm audit --audit-level critical --json > npm-audit.json || true
    - npm audit --audit-level critical
    - echo "Checking for known security issues..."
    - |
      CRITICAL_VULNS=$(cat npm-audit.json | jq '.metadata.vulnerabilities.critical // 0')
      HIGH_VULNS=$(cat npm-audit.json | jq '.metadata.vulnerabilities.high // 0')
      echo "Critical vulnerabilities: $CRITICAL_VULNS"
      echo "High vulnerabilities: $HIGH_VULNS"
      if [ "$CRITICAL_VULNS" -gt 0 ]; then
        echo "❌ Critical vulnerabilities found! Please fix before deploying to production."
        exit 1
      fi
  artifacts:
    reports:
      junit: npm-audit.json
    paths:
      - npm-audit.json
    expire_in: 1 week
  allow_failure: true
  rules:
    - if: '$CI_COMMIT_BRANCH == "develop" || $CI_COMMIT_BRANCH == "main"'


deploy_frontend_production:
  <<: *production_job_definition
  stage: deploy-production
  image: alpine/k8s:1.30.0
  variables:
    ENVIRONMENT: production
    KUBECONFIG: "${CI_PROJECT_DIR}/.kube/config"
  before_script:
    - apk add --no-cache curl jq gettext bash
    - mkdir -p "$(dirname "$KUBECONFIG")"
    - |
      cat > "${KUBECONFIG}" <<EOF
      apiVersion: v1
      clusters:
      - cluster:
          server: ${K3S_SERVER}
          certificate-authority-data: ${K3S_CA_CERT}
        name: k3s-cluster
      contexts:
      - context:
          cluster: k3s-cluster
          user: k3s-user
        name: k3s-context
      current-context: k3s-context
      kind: Config
      preferences: {}
      users:
      - name: k3s-user
        user:
          client-certificate-data: ${K3S_CLIENT_CERT}
          client-key-data: ${K3S_CLIENT_KEY}
      EOF
    - echo "Testing connection to K3s cluster..."
    - kubectl get nodes
    - kubectl create namespace ${ENVIRONMENT} --dry-run=client -o yaml | kubectl apply -f -
    # Create GitLab container registry secret
    - |
      kubectl delete secret gitlab-registry-secret -n ${ENVIRONMENT} --ignore-not-found=true
      kubectl create secret docker-registry gitlab-registry-secret \
        --docker-server=$GITLAB_REGISTRY_URL \
        --docker-username=$GITLAB_REGISTRY_USER \
        --docker-password=$GITLAB_REGISTRY_PASSWORD \
        -n ${ENVIRONMENT} \
        --dry-run=client -o yaml | kubectl apply -f -
    # Install Kustomize
    - curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
    - mv kustomize /usr/local/bin/
    - kustomize version
  script:
    - echo "Building frontend production manifests with Kustomize..."
    - cd k8s/overlays/production
    - kustomize edit fix  # ensure compatibility
    - |
      echo "=== UPDATING FRONTEND IMAGE TAG FOR PRODUCTION ==="
      echo "Setting image to: ${CI_REGISTRY_IMAGE}:${CI_COMMIT_SHORT_SHA}"
      kustomize edit set image monitoring-frontend=${CI_REGISTRY_IMAGE}:${CI_COMMIT_SHORT_SHA}
    
    - kustomize build . > ${CI_PROJECT_DIR}/k8s-frontend-production.yaml
    - echo "Applying frontend manifests to production..."
    - kubectl apply -f ${CI_PROJECT_DIR}/k8s-frontend-production.yaml
    - echo "Waiting for frontend rollout to finish..."
    - DEPLOYMENT_NAME=$(kubectl get deployment -n ${ENVIRONMENT} -l app=monitoring-frontend -o jsonpath='{.items[0].metadata.name}')
    - kubectl rollout status deployment/$DEPLOYMENT_NAME -n ${ENVIRONMENT} --timeout=600s
    - echo "=== FINAL FRONTEND STATUS ==="
    - kubectl get all -n ${ENVIRONMENT} -l app=monitoring-frontend
    - kubectl get ingress -n ${ENVIRONMENT} -l app=monitoring-frontend
    - echo "Frontend deployment complete."
  rules:
    - if: '$CI_COMMIT_BRANCH == "develop" || $CI_COMMIT_BRANCH == "main"'
  
  artifacts:
    paths:
      - k8s-frontend-production.yaml
    expire_in: 1 week
    when: always

verify_frontend_deployment:
  <<: *production_job_definition
  stage: monitoring-verify
  image: alpine/k8s:1.30.0
  variables:
    ENVIRONMENT: production
    KUBECONFIG: "${CI_PROJECT_DIR}/.kube/config"
    FRONTEND_URL: "http://monitoring-frontend.production.local"  # Adjust to your actual domain
    
  before_script:
    - apk add --no-cache curl jq bash
    - mkdir -p "$(dirname "$KUBECONFIG")"
    - echo "${PRODUCTION_VM_IP} monitoring-frontend.production.local" >> /etc/hosts  # Adjust domain
    # Setup kubeconfig (same as deploy job)
    - |
      cat > "${KUBECONFIG}" <<EOF
      apiVersion: v1
      clusters:
      - cluster:
          server: ${K3S_SERVER}
          certificate-authority-data: ${K3S_CA_CERT}
        name: k3s-cluster
      contexts:
      - context:
          cluster: k3s-cluster
          user: k3s-user
        name: k3s-context
      current-context: k3s-context
      kind: Config
      users:
      - name: k3s-user
        user:
          client-certificate-data: ${K3S_CLIENT_CERT}
          client-key-data: ${K3S_CLIENT_KEY}
      EOF
  
  script:
    - echo "=== VERIFYING FRONTEND DEPLOYMENT ==="
    
    # Check if frontend pods are running
    - |
      echo "Checking frontend pod status..."
      kubectl get pods -n ${ENVIRONMENT} -l app=monitoring-frontend
      READY_PODS=$(kubectl get pods -n ${ENVIRONMENT} -l app=monitoring-frontend -o jsonpath='{.items[*].status.conditions[?(@.type=="Ready")].status}' | grep -c "True" || echo "0")
      TOTAL_PODS=$(kubectl get pods -n ${ENVIRONMENT} -l app=monitoring-frontend --no-headers | wc -l)
      echo "Ready pods: $READY_PODS/$TOTAL_PODS"
      
      if [ "$READY_PODS" -eq "0" ]; then
        echo "❌ No frontend pods are ready"
        kubectl describe pods -n ${ENVIRONMENT} -l app=monitoring-frontend
        exit 1
      else
        echo "✅ Frontend pods are ready"
      fi
    
    # Check service
    - |
      echo "Checking frontend service..."
      kubectl get service -n ${ENVIRONMENT} monitoring-frontend-service
      SERVICE_ENDPOINTS=$(kubectl get endpoints -n ${ENVIRONMENT} monitoring-frontend-service -o jsonpath='{.subsets[*].addresses[*].ip}' | wc -w)
      echo "Service endpoints: $SERVICE_ENDPOINTS"
      
      if [ "$SERVICE_ENDPOINTS" -eq "0" ]; then
        echo "❌ No service endpoints available"
        exit 1
      else
        echo "✅ Service has endpoints"
      fi
    
    # Check ingress
    - |
      echo "Checking frontend ingress..."
      kubectl get ingress -n ${ENVIRONMENT} monitoring-frontend-ingress || echo "⚠️ Ingress not found"
    
    # Test frontend health endpoint
    - |
      echo "Testing frontend health endpoint..."
      sleep 10  # Give some time for services to be ready
      
      # Try internal service first
      if kubectl exec -n ${ENVIRONMENT} deployment/monitoring-frontend -- wget -q -O- http://localhost:8080/health 2>/dev/null; then
        echo "✅ Frontend health check (internal) successful"
      else
        echo "❌ Frontend health check (internal) failed"
      fi
    
    # Test frontend main page
    - |
      echo "Testing frontend main page..."
      if kubectl exec -n ${ENVIRONMENT} deployment/monitoring-frontend -- wget -q -O- http://localhost:8080/ 2>/dev/null | head -20; then
        echo "✅ Frontend main page accessible"
      else
        echo "❌ Frontend main page not accessible"
      fi
    
    # Test external access (if ingress is configured)
    - |
      if curl -f -s --connect-timeout 10 "${FRONTEND_URL}/health" > /dev/null; then
        echo "✅ Frontend externally accessible"
        # Test if it can reach the backend API
        if curl -f -s --connect-timeout 10 "${FRONTEND_URL}/api/health" > /dev/null; then
          echo "✅ Frontend can proxy to backend API"
        else
          echo "⚠️ Frontend cannot reach backend API (this might be expected if backend is not ready)"
        fi
      else
        echo "⚠️ Frontend not externally accessible (check ingress configuration)"
      fi
    
    # Check logs for any errors
    - |
      echo "Checking frontend logs for errors..."
      kubectl logs -n ${ENVIRONMENT} -l app=monitoring-frontend --tail=50 --since=5m | grep -i error || echo "No errors found in recent logs"
    
    - echo "=== FRONTEND VERIFICATION COMPLETE ==="
    - echo "Frontend pods status:"
    - kubectl get pods -n ${ENVIRONMENT} -l app=monitoring-frontend
    - echo "Frontend service status:"
    - kubectl get service -n ${ENVIRONMENT} monitoring-frontend-service
  
  rules:
    - if: '$CI_COMMIT_BRANCH == "develop" || $CI_COMMIT_BRANCH == "main"'
  needs:
    - job: deploy_frontend_production
      optional: false


deploy_monitoring_frontend_gke:
  <<: *monitoring_gke_deploy
  stage: deploy-gke
  variables:
    GKE_FRONTEND_NAMESPACE: "monitoring-frontend"
    ENVIRONMENT: gke
    KUBECONFIG: "${CI_PROJECT_DIR}/.kube/gke-config"
    USE_GKE_GCLOUD_AUTH_PLUGIN: "True"
    CLOUDSDK_CORE_PROJECT: "$GCP_PROJECT_ID"
  script:
    - echo "=== Context ==="
    - echo "Project $GCP_PROJECT_ID"
    - echo "Cluster $GKE_CLUSTER_NAME"
    - echo "Zone $GKE_ZONE"
    - echo "Namespace $GKE_FRONTEND_NAMESPACE"

    # Create namespace if it doesn't exist
    - echo "Creating namespace if it doesn't exist..."
    - kubectl create namespace "$GKE_FRONTEND_NAMESPACE" --dry-run=client -o yaml | kubectl apply -f -

    # Create GitLab registry secret for GKE
    - echo "Creating GitLab Container Registry secret for GKE..."
    - |
      kubectl delete secret gitlab-registry-secret -n ${GKE_FRONTEND_NAMESPACE} --ignore-not-found=true
      kubectl create secret docker-registry gitlab-registry-secret \
        --docker-server=$CI_REGISTRY \
        --docker-username=$CI_REGISTRY_USER \
        --docker-password=$GITLAB_PERSONAL_TOKEN \
        -n ${GKE_FRONTEND_NAMESPACE} \
        --dry-run=client -o yaml | kubectl apply -f -

    # Patch image tag with commit SHA
    - ls 
    - echo "Patching image with SHA $CI_COMMIT_SHORT_SHA"
    - cd k8s/overlays/gcp
    - kustomize edit set image monitoring-frontend=registry.proxiad-axe-seine.com/proxiad-academie/ops-dashboard-ui:$CI_COMMIT_SHORT_SHA
    - cd -

    # Kustomize build and apply
    - echo "Building Kustomize overlay for monitoring-frontend..."
    - kustomize build k8s/overlays/gcp | tee monitoring-frontend-kustomize-output.yaml | kubectl apply -f -

    # Ensure ManagedCertificate exists (optional, if needed)
    - echo "Ensuring ManagedCertificate exists..."
    - |
      if ! kubectl get managedcertificate monitoring-frontend-ssl-cert -n "$GKE_FRONTEND_NAMESPACE" > /dev/null 2>&1; then
        echo "Creating ManagedCertificate..."
        kubectl apply -f k8s/overlays/gcp/managed-certificate.yaml
      else
        echo "ManagedCertificate already exists. Skipping creation."
      fi

    # Ensure Ingress exists (optional)
    - echo "Ensuring Ingress exists..."
    - |
      if ! kubectl get ingress monitoring-frontend-ingress -n "$GKE_FRONTEND_NAMESPACE" > /dev/null 2>&1; then
        echo "Creating Ingress..."
        kubectl apply -f k8s/overlays/gcp/gcp-ingress.yaml
      else
        echo "Ingress already exists. Skipping creation."
      fi

    # Wait for rollout to complete
    - echo "Waiting for rollout to complete..."
    - kubectl rollout status deployment/monitoring-frontend -n "$GKE_FRONTEND_NAMESPACE" --timeout=300s

    # Debug
    - echo "=== Resources ==="
    - kubectl get all -n "$GKE_FRONTEND_NAMESPACE"
    - echo "=== Describe pods ==="
    - kubectl describe pods -l app=monitoring-frontend -n "$GKE_FRONTEND_NAMESPACE"
    - echo "=== Logs (latest) ==="
    - kubectl logs -l app=monitoring-frontend -n "$GKE_FRONTEND_NAMESPACE" --tail=100 --ignore-errors

  rules:
    - if: $CI_COMMIT_BRANCH == "develop" || $CI_COMMIT_BRANCH == "main"



lighthouse_audit:
  <<: *monitoring_gke_deploy
  stage: test
  image: node:18-slim
  variables:
    LIGHTHOUSE_URL: "https://monitoring-dashboard.duckdns.org/public-health"  # ✅ Replace with your public route if changed
  before_script:
    - apt-get update && apt-get install -y curl unzip wget jq
    - npm install -g @lhci/cli
    - echo "Downloading Chromium..."
    - export CHROME_URL="https://storage.googleapis.com/chrome-for-testing-public/124.0.6367.208/linux64/chrome-linux64.zip"
    - wget -qO chrome.zip "$CHROME_URL"
    - unzip chrome.zip
    - export CHROME_PATH=$(pwd)/chrome-linux64/chrome
    - echo "✅ Chromium path $CHROME_PATH"
  script:
    - echo "Running Lighthouse audit on $LIGHTHOUSE_URL"
    - lhci collect --url="$LIGHTHOUSE_URL" \
        --settings.chromePath="$CHROME_PATH" \
        --settings.chromeFlags="--headless --no-sandbox --disable-gpu" \
        --outputPath="./.lighthouseci/report.html"
    - echo "✅ Lighthouse audit complete"
  artifacts:
    paths:
      - .lighthouseci/
    expire_in: 1 week
  allow_failure: true
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" || $CI_COMMIT_BRANCH == "develop"'
